---
title: "Mushroom Classification"
author: "Zahraa Alshalal"
date: "2023-05-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Libraries
# packages
library(dplyr)
library(plotly)
library(tidyverse)
library(MASS)
library(DataExplorer)
library(Hmisc)
library(polycor)
library(corrplot)
library(htmlwidgets)
library(moderndive)
library(leaps)
library('IRdisplay')
library(pROC)
library(car)
library(DiagrammeR)
library(plyr)
library(caret)
```

### 1. Problem statement and dataset description:

-   The problem is to build a classification model that can predict whether a mushroom is edible or poisonous based on its physical attributes such as cap shape, cap color, odor, and more. The data-set used for this analysis is the Mushroom Classification dataset available on the UCI Machine Learning Repository. The data-set contains 8124 observations of mushrooms, with 23 features including the class (edible or poisonous).

```{r}
mushroom = read.csv("/Users/zahraaalshalal/Desktop/spring23/math449/finalproject/z_analysis/mushrooms.csv", header=TRUE)
glimpse(mushroom)
```

### 2. Fitting a logistic regression model with all predictors:

-   Fitting the model on the entire data-set be over fitting to the training data and the performance of the model on new, unseen data may not be as good. It's generally a good practice to split the data into training and testing sets before fitting the model.

```{r}
# Convert the 'class' column to a factor variable
mushroom$class = as.factor(mushroom$class)

# Split the data into training and testing sets
library(caTools)
set.seed(123)
split = sample.split(mushroom$class, SplitRatio = 0.7)
train = subset(mushroom, split == TRUE)
test = subset(mushroom, split == FALSE)

# Encode all other categorical variables in the training set
for (col in names(train)[2:length(names(train))]) {
  train[, col] <- as.numeric(factor(train[, col]))
}

# Encode all other categorical variables in the testing set
for (col in names(test)[2:length(names(test))]) {
  test[, col] <- as.numeric(factor(test[, col]))
}
# Exclude:- veil-type (as it has only one type i.e. ‘partial’.)
train <- train[, -which(names(train) == "veil.type")]
test <- test[, -which(names(test) == "veil.type")]
#glimpse(train)
```

```{r}
# logistic regression model
model = glm(class ~ ., data = train, family = "binomial")
#summary(model)
```

### 3. Select the best subset of variables. Perform a diagnostic on the best model. Perform all possible inferences you can think about.

```{r}
# perform forward stepwise selection on the training data
step.model = step(model, direction = "forward", trace = FALSE)
summary(step.model)
```

-   Variance Inflation Factor (VIF) is a measure of multicollinearity among the independent variables in a regression model.

```{r}
# calculate the VIF values for the model 
vif = vif(model)
vif
```

### 4. Use the new model to make predictions.

```{r}
model = glm(class ~ cap.shape + cap.surface + cap.color + bruises + odor + gill.attachment + gill.color + stalk.shape + stalk.surface.below.ring + stalk.color.above.ring + stalk.color.below.ring + veil.color + ring.number + ring.type + spore.print.color + population + habitat, family = "binomial", data = train)


# Predictions

# make predictions on the training data
train$predicted <- predict(model, newdata = train, type = "response")
# compute the confusion matrix on the training data
train$predicted_class <- ifelse(train$predicted > 0.5, "p", "e")
confusion_train <- table(Actual = train$class, Predicted = train$predicted_class)
cat("Training Confusion Matrix:\n")
print(confusion_train)


# make predictions on the testing data
test$predicted <- predict(model, newdata = test, type = "response")
# compute the confusion matrix on the testing data
test$predicted_class <- ifelse(test$predicted > 0.5, "p", "e")
confusion_test <- table(Actual = test$class, Predicted = test$predicted_class)
cat("\nTesting Confusion Matrix:\n")
print(confusion_test)
```

### 5. Use different pi_0 as a cut-off point and create a confusion table.

-   Dropping any feature with above 10 value. A VIF value of 10 or above is usually considered very high and may indicate a serious problem with multicollinearity.

-   These feature selected: "cap.shape + cap.surface + cap.color + bruises + odor + gill.attachment + gill.color + stalk.shape + stalk.surface.below.ring + stalk.color.above.ring + stalk.color.below.ring + veil.color + ring.number + ring.type + spore.print.color + population + habitat"

```{r}
# Create a new model with only the variables that have a VIF value below a threshold = 10
model = glm(class ~ cap.shape + cap.surface + cap.color + bruises + odor + gill.attachment + gill.color + stalk.shape + stalk.surface.below.ring + stalk.color.above.ring + stalk.color.below.ring + veil.color + ring.number + ring.type + spore.print.color + population + habitat, family = "binomial", data = train)

# Define a vector of pi_0 values as cut-off points
pi_0_vec <- seq(0.1, 0.9, by = 0.1)

# Create an empty list to store the confusion matrices for each pi_0 value
conf_mat_list <- list()

for (pi_0 in pi_0_vec) {
  predictions <- predict(model, newdata = test, type = "response")
  pred_class <- ifelse(predictions > pi_0, "p", "e")
  conf_mat <- table(Predicted = pred_class, Actual = test$class)
  colnames(conf_mat) <- c("Edible", "Poisonous")
  rownames(conf_mat) <- c("Edible", "Poisonous")
  conf_mat_list[[as.character(pi_0)]] <- conf_mat
}

for (pi_0 in pi_0_vec) {
  cat("Confusion matrix for pi_0 =", pi_0, ":\n")
  print(conf_mat_list[[as.character(pi_0)]])
  cat("\n")
}
# Vector of pi_0 cutoff values to try
pi0_cutoffs <- seq(0.1, 0.9, by = 0.1)

# Initialize an empty vector to store accuracy values
accuracy_vec <- numeric(length = length(pi0_cutoffs))

# Loop over each pi_0 cutoff value and create a confusion matrix
for (i in seq_along(pi0_cutoffs)) {
  pred_class <- ifelse(predictions > pi0_cutoffs[i], "p", "e")
  conf_mat <- table(Predicted = pred_class, Actual = test$class)
  accuracy_vec[i] <- sum(diag(conf_mat)) / sum(conf_mat)
}

```

```{r}
#For pi_0 = 0.5:

Accuracy1 = (1147 + 1014) / (1147 + 161 + 115 + 1014) 
Precision1 = 1147 / (1147 + 115) 
Recall1 = 1147 / (1147 + 161) 
F1_score1 = 2 * Precision1 * Recall1 / (Precision1 + Recall1) 

#For pi_0 = 0.6:
Accuracy2 = (1176 + 987) / (1176 + 188 + 86 + 987) 
Precision2 = 1176 / (1176 + 86)
Recall2 = 1176 / (1176 + 188) 
F1_score2 = 2 * Precision2 * Recall2 / (Precision2 + Recall2) 

# create a data frame with the results
df <- data.frame(
  pi_0 = c(0.5, 0.6),
  Accuracy = c(Accuracy1, Accuracy2),
  Precision = c(Precision1, Precision2),
  Recall = c(Recall1, Recall2),
  F1_Score = c(F1_score1, F1_score2)
)

# display the table using knitr::kable()
knitr::kable(df, align = "c", digits = 2)

```

-   For pi_0 = 0.5, the accuracy and recall are high, indicating that the classifier is able to correctly classify most instances and has a low false negative rate. However, the precision is slightly lower, indicating that there are some false positive classifications. The F1 score, which balances precision and recall, is high, indicating a good overall performance.

-   For pi_0 = 0.6, the precision is higher, indicating that there are fewer false positive classifications, but the recall is lower, indicating that there are more false negatives. The accuracy and F1 score are both still high, indicating a good overall performance.

### 6. Perform visualization of data and models.

```{r}
gill <- table(mushroom$class, mushroom$`gill.size`)
barplot(gill, beside = T, legend = rownames(mushroom$`gill.size`),
        xlab = "Gill Size", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
odor <- table(mushroom$class, mushroom$odor)
barplot(odor, beside = T, legend = rownames(mushroom$odor),
        xlab = "Odor", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
spore <- table(mushroom$class, mushroom$`spore.print.colo`)
barplot(gill, beside = T, legend = rownames(mushroom$`spore.print.color`),
        xlab = "Spore Print Colour", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
ring <- table(mushroom$class, mushroom$`ring.type`)
barplot(ring, beside = T, legend = rownames(mushroom$`ring.type`),
        xlab = "Ring Type", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
population <- table(mushroom$class, mushroom$population)
barplot(population, beside = T, legend = rownames(mushroom$population),
        xlab = "Population", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
habitat <- table(mushroom$class, mushroom$habitat)
barplot(habitat, beside = T, legend = rownames(mushroom$population),
        xlab = "Habitate", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

-   The important features of mushrooms to identify that it is safe to eat are as follows :-\
-   Odor -- creosote, foul, musty, pungent, spicy and fishy.
-   Gill Size -- only Narrow.
-   Rings -- whether large or absent.
-   Spore print color -- only brown and not black.
-   Population -- available in several places.
-   Habitat -- grown on leaves, path and urban.

### 7. Plot the ROC curve, find AUC, and the best cutoff point for classification.

```{r}
#AUC (Area Under the Curve), ROC (Receiver Operating Characteristic) 
# predict class probabilities for the test set
probabilities <- predict(model, newdata = test, type = "response")

# calculate FPR and TPR for various threshold values
roc_data <- pROC::roc(test$class, probabilities)

# plot the ROC curve
plot(roc_data, main = "ROC Curve")

# calculate AUC
auc <- pROC::auc(roc_data)
cat(sprintf("AUC: %.2f\n", auc))

# calculate the best cutoff point
cutoff <- pROC::coords(roc_data, "best", ret = "threshold")[[1]]
cat(sprintf("Best cutoff point: %.2f\n", cutoff))

# Choose a cutoff point
cutoff <- 0.62

# Calculate predicted probabilities for the test set
test_prob <- predict(model, type="response", newdata=test)

# Convert probabilities to predicted classes using the cutoff
test_pred <- ifelse(test_prob > cutoff, 1, 0)

# Create confusion matrix
confusion <- table(test$class, test_pred)

# Calculate sensitivity and specificity
sensitivity <- confusion[2,2]/sum(confusion[2,])
specificity <- confusion[1,1]/sum(confusion[1,])

# Find the pi_0 cutoff value that gives the highest accuracy
best_cutoff <- pi0_cutoffs[which.max(accuracy_vec)]

cat("Best cutoff:", round(best_cutoff, 2), "\n")
cat("Sensitivity:", round(sensitivity, 2), "\n")
cat("Specificity:", round(specificity, 2), "\n")
```

-   The AUC of the model is 0.94, indicating good performance in distinguishing between edible and poisonous mushrooms.

-   The best cutoff point for classification is 0.62, which maximizes the trade-off between sensitivity and specificity. The sensitivity of the model is 0.84. This means that it correctly identifies 84% of the positive cases, while the specificity is 0.94, indicating that it correctly identifies 94% of the negative cases.

### 8. Perform LOOCV and k-fold cross-validation.

```{r}
# LOOCV
#library(caret)
#control = trainControl(method="LOOCV")
#model = train(class ~ cap.shape + cap.surface + cap.color + bruises + odor + gill.attachment + gill.color + stalk.shape + stalk.surface.below.ring + stalk.color.above.ring + stalk.color.below.ring + veil.color + ring.number + ring.type + spore.print.color + population + habitat, data=train, method="glm", family="binomial", trControl=control)
#summary(model)
```

```{r}
# k-fold Cross-Validation:
#control <- trainControl(method="cv", number=10)
#model <- train(class ~ cap.shape + cap.surface + cap.color + bruises + odor + gill.attachment + gill.color + stalk.shape + stalk.surface.below.ring + stalk.color.above.ring + stalk.color.below.ring + veil.color + ring.number + ring.type + spore.print.color + population + habitat, data=train, method="glm", family="binomial", trControl=control)
#summary(model)

```
